{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMypml5xScT5gHkhHCiN5w2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaixih/JAX101/blob/master/pjit_flax_logical_axis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WAC1aOY8PAGQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
        "import jax\n",
        "\n",
        "from jax import lax, random, numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax import struct, traverse_util, linen as nn\n",
        "from flax.linen import spmd # Flax Linen SPMD."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental.pjit import pjit, with_sharding_constraint\n",
        "from jax.sharding import Mesh, PartitionSpec\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "# Start a device mesh.\n",
        "device_mesh = mesh_utils.create_device_mesh((4, 2))\n",
        "\n",
        "# Annotate each axis with a name.\n",
        "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\n",
        "mesh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TMOW0AZPJd_",
        "outputId": "404deeaf-6347-4500-8d4d-ecedfaf16db8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mesh(device_ids=array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7]]), axis_names=('data', 'model'))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SuperDot(nn.Module):\n",
        "  depth: int\n",
        "  max_history_length: int\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    W1 = self.param(\n",
        "        'W1', \n",
        "        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('embed', 'hidden')),\n",
        "        (x.shape[-1], self.depth))\n",
        "    x_max_history = self.variable(\n",
        "        'fp8_params', 'x_max_history',\n",
        "        nn.with_partitioning(nn.initializers.zeros_init(), (None,)),\n",
        "        self.make_rng('fp8_params'), (self.max_history_length,))\n",
        "    x_scale = self.variable(\n",
        "        'fp8_params', 'x_scale', \n",
        "        nn.with_partitioning(nn.initializers.ones_init(), (None,)),\n",
        "        self.make_rng('fp8_params'), (1,))\n",
        "    \n",
        "    # Mimic the dequantization.\n",
        "    y = jnp.dot(x * x_scale.value, W1)\n",
        "    x_max = jnp.max(x, axis=(0, 1), keepdims=True)\n",
        "\n",
        "    # Mimic the update of the max_history and the scale.\n",
        "    x_max_history.value = x_max_history.value.at[0].set(x_max[0 ,0])\n",
        "    x_max_history.value = jnp.roll(x_max_history.value, -1, axis=0)\n",
        "    value_max = jax.nn.initializers.constant(57344.)(self.make_rng('fp8_params'), (1,), jnp.float32)\n",
        "    x_scale.value = value_max / jnp.max(x_max_history.value)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "P5RYkEOzPLj_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\n",
        "k = random.PRNGKey(0)\n",
        "\n",
        "model = SuperDot(8192, 16)"
      ],
      "metadata": {
        "id": "Ccbs6mhCPuUb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A functional way of model initialization.\n",
        "def init_fn(k, x):\n",
        "  rngs = {'params': k, 'fp8_params': k}\n",
        "  variables = model.init(rngs, x) # Initialize the model.\n",
        "  return variables\n",
        "\n",
        "abstract_variables = jax.eval_shape(init_fn, k, x)\n",
        "# This `state_spec` has the same pytree structure as the output\n",
        "# of the `init_fn`.\n",
        "logical_output_spec = nn.get_partition_spec(abstract_variables)\n",
        "logical_output_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Vf1u2CPwdR",
        "outputId": "15ff099a-1658-466a-8182-be3072b1ad31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    fp8_params: {\n",
              "        x_max_history: PartitionSpec(None,),\n",
              "        x_scale: PartitionSpec(None,),\n",
              "    },\n",
              "    params: {\n",
              "        W1: PartitionSpec('embed', 'hidden'),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unspecified rule means unsharded by default, so no need to specify `('embed', None)` and `('layer', None)`.\n",
        "rules = (('batch', 'data'),\n",
        "         ('hidden', 'model'))\n",
        "\n",
        "logical_state_spec = spmd.logical_to_mesh(logical_output_spec, rules)\n",
        "logical_state_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6RgcYfP0vi",
        "outputId": "447d1122-0e55-4c8e-cf04-fbf7d0d82dcf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    fp8_params: {\n",
              "        x_max_history: PartitionSpec(None,),\n",
              "        x_scale: PartitionSpec(None,),\n",
              "    },\n",
              "    params: {\n",
              "        W1: PartitionSpec(None, 'model'),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pjit_init_fn = pjit(init_fn,\n",
        "                    in_axis_resources=(PartitionSpec(None), PartitionSpec('data', None)),  # PRNG key and x\n",
        "                    out_axis_resources=logical_state_spec,  # params\n",
        "                    )\n",
        "# if in_axis_resources, we need mesh context\n",
        "with mesh:\n",
        "  initialized_state = pjit_init_fn(k, x)\n",
        "jax.tree_map(jnp.shape, initialized_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkOnMFPeQCAh",
        "outputId": "45e00f53-f50f-421a-911a-d8fd843e9116"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    fp8_params: {\n",
              "        x_max_history: Partitioned(value=(16,), names=(None,), mesh=None),\n",
              "        x_scale: Partitioned(value=(1,), names=(None,), mesh=None),\n",
              "    },\n",
              "    params: {\n",
              "        W1: LogicallyPartitioned(value=(8192, 8192), names=('embed', 'hidden'), mesh=None, rules=None),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(initialized_state['params']['W1'].value.sharding)\n",
        "print(initialized_state['fp8_params']['x_max_history'].value.sharding)\n",
        "print(initialized_state['fp8_params']['x_scale'].value.sharding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XROVLGyQLp2",
        "outputId": "a93fe31f-0be8-441e-a7f5-ea7d1ba928da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GSPMDSharding({devices=[1,2,4]0,2,4,6,1,3,5,7 last_tile_dim_replicate})\n",
            "GSPMDSharding({replicated})\n",
            "GSPMDSharding({replicated})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_step(state, x):\n",
        "  rngs = {'fp8_params': jax.random.PRNGKey(0)}\n",
        "  y, new_state = model.apply({'params': state['params'], 'fp8_params': state['fp8_params']}, x, rngs=rngs, mutable=['fp8_params'])\n",
        "  return y, new_state['fp8_params']\n",
        "\n",
        "pjit_step_fn = pjit(infer_step,\n",
        "                    in_axis_resources=(logical_state_spec, PartitionSpec('data', None)),  # params and x\n",
        "                    out_axis_resources=(PartitionSpec('data', 'model'), PartitionSpec(None))  # y and max\n",
        "                    )\n",
        "with mesh:\n",
        "  y, new_state = pjit_step_fn(initialized_state, x)\n",
        "print('y sharding:')\n",
        "jax.debug.visualize_array_sharding(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "XJwwSZbdQOgx",
        "outputId": "7c6982f0-6074-4eb5-a4ab-382bee17dbe8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y sharding:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m   \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m   \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m   \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m    \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m   \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">   CPU 1    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">   CPU 2    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 3    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">   CPU 4    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">   CPU 5    </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">   CPU 6    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">   CPU 7    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mesh:\n",
        "  lowered = pjit_step_fn.lower(initialized_state, x)\n",
        "compiled = lowered.compile().compiler_ir()"
      ],
      "metadata": {
        "id": "7XiGLvKZRD5l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for module in compiled:\n",
        "  print(module.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9BKqmAXRFU-",
        "outputId": "b5214a3c-4eeb-415e-80b6-0a978ee6035f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HloModule pjit_infer_step, entry_computation_layout={(f32[16]{0},f32[1]{0},f32[8192,4096]{1,0},f32[2048,8192]{1,0})->(f32[2048,4096]{1,0}, f32[16]{0}, f32[1]{0})}, allow_spmd_sharding_propagation_to_output={false,false,false}\n",
            "\n",
            "%region_0.16 (Arg_0.17: f32[], Arg_1.18: f32[]) -> f32[] {\n",
            "  %Arg_0.17 = f32[] parameter(0)\n",
            "  %Arg_1.18 = f32[] parameter(1)\n",
            "  ROOT %maximum.19 = f32[] maximum(f32[] %Arg_0.17, f32[] %Arg_1.18), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0, 1)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=21}\n",
            "}\n",
            "\n",
            "%region_2.62 (Arg_0.63: f32[], Arg_1.64: f32[]) -> f32[] {\n",
            "  %Arg_0.63 = f32[] parameter(0)\n",
            "  %Arg_1.64 = f32[] parameter(1)\n",
            "  ROOT %maximum.65 = f32[] maximum(f32[] %Arg_0.63, f32[] %Arg_1.64), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0,)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=27}\n",
            "}\n",
            "\n",
            "%fused_computation (param_0.3: f32[32]) -> f32[1] {\n",
            "  %constant.65 = f32[1]{0} constant({57344})\n",
            "  %param_0.3 = f32[32]{0} parameter(0)\n",
            "  %slice.7 = f32[16]{0} slice(f32[32]{0} %param_0.3), slice={[1:17]}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/dynamic_slice[slice_sizes=(16,)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "  %constant.66 = f32[] constant(-inf)\n",
            "  %reduce.5 = f32[] reduce(f32[16]{0} %slice.7, f32[] %constant.66), dimensions={0}, to_apply=%region_2.62, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0,)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=27}\n",
            "  %reshape.38 = f32[1]{0} reshape(f32[] %reduce.5), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/div\" source_file=\"<ipython-input-3-640b48752345>\" source_line=27}\n",
            "  ROOT %divide.1 = f32[1]{0} divide(f32[1]{0} %constant.65, f32[1]{0} %reshape.38), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/div\" source_file=\"<ipython-input-3-640b48752345>\" source_line=27}\n",
            "}\n",
            "\n",
            "%fused_computation.1 (param_0.5: f32[16]) -> f32[16] {\n",
            "  %param_0.5 = f32[16]{0} parameter(0)\n",
            "  %concatenate.3 = f32[32]{0} concatenate(f32[16]{0} %param_0.5, f32[16]{0} %param_0.5), dimensions={0}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/concatenate[dimension=0]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "  ROOT %slice.8 = f32[16]{0} slice(f32[32]{0} %concatenate.3), slice={[1:17]}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/dynamic_slice[slice_sizes=(16,)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "}\n",
            "\n",
            "%fused_computation.2 (param_0.7: f32[16], param_1.6: f32[]) -> f32[32] {\n",
            "  %param_0.7 = f32[16]{0} parameter(0)\n",
            "  %param_1.6 = f32[] parameter(1)\n",
            "  %reshape.39 = f32[1]{0} reshape(f32[] %param_1.6), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0, 1)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=21}\n",
            "  %constant.67 = s32[] constant(0)\n",
            "  %dynamic-update-slice.2 = f32[16]{0} dynamic-update-slice(f32[16]{0} %param_0.7, f32[1]{0} %reshape.39, s32[] %constant.67), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=24}\n",
            "  ROOT %concatenate.4 = f32[32]{0} concatenate(f32[16]{0} %dynamic-update-slice.2, f32[16]{0} %dynamic-update-slice.2), dimensions={0}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/concatenate[dimension=0]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "}\n",
            "\n",
            "%fused_computation.3 (param_0.8: f32[16], param_1.9: f32[]) -> f32[16] {\n",
            "  %param_0.8 = f32[16]{0} parameter(0)\n",
            "  %param_1.9 = f32[] parameter(1)\n",
            "  %reshape.40 = f32[1]{0} reshape(f32[] %param_1.9), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0, 1)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=21}\n",
            "  %constant.68 = s32[] constant(0)\n",
            "  ROOT %dynamic-update-slice.3 = f32[16]{0} dynamic-update-slice(f32[16]{0} %param_0.8, f32[1]{0} %reshape.40, s32[] %constant.68), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=24}\n",
            "}\n",
            "\n",
            "%fused_computation.4.clone (param_0.10: f32[2048,8192], param_1.13: f32[1]) -> f32[2048,8192] {\n",
            "  %param_0.10 = f32[2048,8192]{1,0} parameter(0)\n",
            "  %param_1.13 = f32[1]{0} parameter(1)\n",
            "  %reshape.42 = f32[] reshape(f32[1]{0} %param_1.13), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/mul\" source_file=\"<ipython-input-3-640b48752345>\" source_line=20}\n",
            "  %broadcast.12 = f32[2048,8192]{1,0} broadcast(f32[] %reshape.42), dimensions={}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/mul\" source_file=\"<ipython-input-3-640b48752345>\" source_line=20}\n",
            "  ROOT %multiply.2 = f32[2048,8192]{1,0} multiply(f32[2048,8192]{1,0} %param_0.10, f32[2048,8192]{1,0} %broadcast.12), metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/mul\" source_file=\"<ipython-input-3-640b48752345>\" source_line=20}\n",
            "}\n",
            "\n",
            "%parallel_fusion.4 (p: f32[2048,8192], p.1: f32[1]) -> f32[2048,8192] {\n",
            "  %p = f32[2048,8192]{1,0} parameter(0)\n",
            "  %p.1 = f32[1]{0} parameter(1)\n",
            "  ROOT %fusion.4.clone = f32[2048,8192]{1,0} fusion(f32[2048,8192]{1,0} %p, f32[1]{0} %p.1), kind=kLoop, calls=%fused_computation.4.clone, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/mul\" source_file=\"<ipython-input-3-640b48752345>\" source_line=20}, backend_config=\"{\\\"outer_dimension_partitions\\\":[\\\"2\\\"]}\"\n",
            "}\n",
            "\n",
            "ENTRY %main.73_spmd (param.3: f32[16], param.1: f32[1], param.2: f32[8192,4096], param: f32[2048,8192]) -> (f32[2048,4096], f32[16], f32[1]) {\n",
            "  %param = f32[2048,8192]{1,0} parameter(3), sharding={devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "  %param.1 = f32[1]{0} parameter(1), sharding={replicated}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/sharding_constraint[sharding=GSPMDSharding({replicated}) resource_env=ResourceEnv(Mesh(device_ids=array([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]]), axis_names=(\\'data\\', \\'model\\')), ()) unconstrained_dims=set()]\" source_file=\"/usr/local/lib/python3.9/dist-packages/flax/core/meta.py\" source_line=243}\n",
            "  %call.1 = f32[2048,8192]{1,0} call(f32[2048,8192]{1,0} %param, f32[1]{0} %param.1), to_apply=%parallel_fusion.4\n",
            "  %param.2 = f32[8192,4096]{1,0} parameter(2), sharding={devices=[1,2,4]0,2,4,6,1,3,5,7 last_tile_dim_replicate}\n",
            "  %dot = f32[2048,4096]{1,0} dot(f32[2048,8192]{1,0} %call.1, f32[8192,4096]{1,0} %param.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=20}\n",
            "  %param.3 = f32[16]{0} parameter(0), sharding={replicated}, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/sharding_constraint[sharding=GSPMDSharding({replicated}) resource_env=ResourceEnv(Mesh(device_ids=array([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]]), axis_names=(\\'data\\', \\'model\\')), ()) unconstrained_dims=set()]\" source_file=\"/usr/local/lib/python3.9/dist-packages/flax/core/meta.py\" source_line=243}\n",
            "  %copy.8 = f32[16]{0} copy(f32[16]{0} %param.3)\n",
            "  %constant.13 = f32[] constant(-inf)\n",
            "  %reduce-window = f32[64,256]{1,0} reduce-window(f32[2048,8192]{1,0} %param, f32[] %constant.13), window={size=32x32 stride=32x32}, to_apply=%region_0.16\n",
            "  %reduce-window.1 = f32[2,8]{1,0} reduce-window(f32[64,256]{1,0} %reduce-window, f32[] %constant.13), window={size=32x32 stride=32x32}, to_apply=%region_0.16\n",
            "  %reduce = f32[] reduce(f32[2,8]{1,0} %reduce-window.1, f32[] %constant.13), dimensions={0,1}, to_apply=%region_0.16, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0, 1)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=21}\n",
            "  %all-reduce = f32[] all-reduce(f32[] %reduce), channel_id=1, replica_groups={{0,2,4,6},{1,3,5,7}}, use_global_device_ids=true, to_apply=%region_0.16, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/reduce_max[axes=(0, 1)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=21}\n",
            "  %fusion.3 = f32[16]{0} fusion(f32[16]{0} %copy.8, f32[] %all-reduce), kind=kLoop, calls=%fused_computation.3, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=24}\n",
            "  %fusion.1 = f32[16]{0} fusion(f32[16]{0} %fusion.3), kind=kLoop, calls=%fused_computation.1, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/dynamic_slice[slice_sizes=(16,)]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "  %fusion.2 = f32[32]{0} fusion(f32[16]{0} %param.3, f32[] %all-reduce), kind=kLoop, calls=%fused_computation.2, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/jit(_roll)/concatenate[dimension=0]\" source_file=\"<ipython-input-3-640b48752345>\" source_line=25}\n",
            "  %fusion = f32[1]{0} fusion(f32[32]{0} %fusion.2), kind=kLoop, calls=%fused_computation, metadata={op_name=\"pjit(infer_step)/jit(main)/SuperDot/div\" source_file=\"<ipython-input-3-640b48752345>\" source_line=27}\n",
            "  ROOT %tuple = (f32[2048,4096]{1,0}, f32[16]{0}, f32[1]{0}) tuple(f32[2048,4096]{1,0} %dot, f32[16]{0} %fusion.1, f32[1]{0} %fusion)\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note\n",
        "\n",
        "\n",
        "*   In `main.spmd`: The `reduce-window` and `reduce` are the hierarchical steps to compute the max of the local ndarray shards, while the `all-reduce` uses collective ops to compute the max globally.\n",
        "*   The `amax_history` update uses two fusion functions: (1) `fusion.3` updates the first element (e.g., `[0,0,0]`->`[x,0,0]`). (2) `fusion.1` roll the amax history by concatenation and slicing (e.g., `[x,0,0]`->`[0,0,0,x,0,0]`->`[0,0,x]`).\n",
        "*   The `scale` update uses two fusion functions: (1) `fusion.2` updates the first element and the concatenation (e.g., `[0,0,0]`->`[x,0,0]`->`[0,0,0,x,0,0]`). (2) `fusion` does the slicing and then reduction and the division (e.g., `[0,0,0,x,0,0]`->`[0,0,x]`->`57344/max([0,0,x])`).\n",
        "\n"
      ],
      "metadata": {
        "id": "yHhl1TT__pzg"
      }
    }
  ]
}